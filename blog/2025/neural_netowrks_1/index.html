<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding neural networks- Part I | Krishnadas Mohandas </title> <meta name="author" content="Krishnadas Mohandas"> <meta name="description" content="Defining the network architecture and intializing the NN parameters"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://krishna-das-m.github.io/blog/2025/neural_netowrks_1/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Krishnadas</span> Mohandas </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding neural networks- Part I</h1> <p class="post-meta"> Created in May 18, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   ·   <a href="/blog/category/ml-concepts"> <i class="fa-solid fa-tag fa-sm"></i> ML-concepts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="step-1-defining-the-neural-network-structure-understanding-layers">Step 1: Defining the Neural Network Structure: Understanding Layers</h2> <p>Before diving into code, it’s essential to understand how a neural network is structured. At its core, a neural network is a system inspired by the human brain, consisting of layers of interconnected “neurons” that process data.</p> <h3 id="1-input-layer">1. <strong>Input Layer</strong> </h3> <p>The input layer is the first layer of the network and serves as the gateway through which raw data enters the model. Each neuron in this layer represents one feature or attribute of the input data.</p> <p>For example, if you’re working with a dataset of handwritten digits (like the popular MNIST dataset), each image is typically 28×28 pixels, resulting in 784 input features (since 28 × 28 = 784). So, your input layer will have 784 neurons, each one receiving the pixel value of the corresponding location in the image.</p> <h3 id="2-hidden-layers">2. <strong>Hidden Layers</strong> </h3> <p>Hidden layers are the intermediate layers between the input and output layers. These layers are where most of the computation happens. Each neuron in a hidden layer takes inputs from all the neurons in the previous layer, applies a weighted sum, adds a bias, and then passes the result through an <strong>activation function</strong> (like ReLU or sigmoid) to introduce non-linearity.</p> <p>The number of hidden layers and the number of neurons in each is up to you. A single hidden layer can work for many simple tasks, but more complex problems often benefit from deeper networks with multiple hidden layers.</p> <h3 id="3-output-layer">3. <strong>Output Layer</strong> </h3> <p>The output layer is the final layer of the network, and its structure depends on the task you’re solving:</p> <ul> <li>For <strong>classification</strong>, the number of neurons equals the number of classes. For example, classifying digits 0–9 means having 10 output neurons, each one representing the probability of a particular digit.</li> <li>For <strong>regression</strong>, typically there’s just one output neuron, giving a continuous numerical value. <h3 id="example-well-use-a-simple-binary-classifier">Example We’ll Use: A Simple Binary Classifier</h3> <p>Let’s say we want to build a neural network that predicts whether a customer will make a purchase based on four input features. Let’s define a few training examples to show what the network will learn from. Each training example consists of 4 input values and 1 target output. In practice, these inputs would be normalized to keep the values between 0 and 1 for better learning performance. A sample part of the data is shown below:</p> </li> </ul> <table> <thead> <tr> <th>Age</th> <th>Income</th> <th>Browsing Time</th> <th>Items Viewed</th> <th>Purchase?</th> </tr> </thead> <tbody> <tr> <td>0.2</td> <td>0.8</td> <td>0.5</td> <td>0.6</td> <td>1</td> </tr> <tr> <td>0.6</td> <td>0.3</td> <td>0.2</td> <td>0.4</td> <td>0</td> </tr> <tr> <td>0.3</td> <td>0.5</td> <td>0.7</td> <td>0.8</td> <td>1</td> </tr> <tr> <td>0.9</td> <td>0.2</td> <td>0.1</td> <td>0.2</td> <td>0</td> </tr> </tbody> </table> <ol> <li> <strong>Age</strong> (normalized between 0 and 1)</li> <li> <strong>Annual Income</strong> (normalized)</li> <li> <strong>Browsing Time on Website</strong> (in minutes, normalized)</li> <li> <strong>Number of Items Viewed</strong> (normalized)</li> </ol> <p>The output will be a binary value:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">1</code> if the customer is likely to make a purchase</li> <li> <code class="language-plaintext highlighter-rouge">0</code> if not <h3 id="network-architecture">Network Architecture</h3> <p>We’ll create a NN architecture as follows:</p> </li> <li> <strong>Input Layer</strong>: 4 neurons (each corresponding to one input feature)</li> <li> <strong>Hidden Layer</strong>: 3 neurons (fully connected to all input neurons)</li> <li> <strong>Output Layer</strong>: 1 neuron (outputs a value between 0 and 1 — interpreted as purchase probability)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/neural_network-480.webp 480w,/assets/img/blog/neural_network-800.webp 800w,/assets/img/blog/neural_network-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/neural_network.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Here the superscript $(m)$ represents the $m^{th}$ training example for the $i^{th}$ input feature. The neurons in the hidden layer $a_i^{[l]}$ represents the activation function of $i^{th}$ neuron in the layer $l$. In general we could represent a neuron in any layer as $a_i^{[l-1]}$ . Great! Now that we’ve defined the structure of the neural network, the next step is to <strong>initialize the parameters</strong> — specifically, the <strong>weights</strong> and <strong>biases</strong>. These parameters are what the network will learn during training.</p> <h2 id="step-2-initializing-parameters--weights-and-biases">Step 2: Initializing Parameters – Weights and Biases</h2> <p>Every connection between neurons in adjacent layers has a <strong>weight</strong>, and every neuron (except those in the input layer) has an associated <strong>bias</strong>. These are the values that the network updates during training to reduce the prediction error.</p> <h3 id="what-are-weights">What Are Weights?</h3> <p>Weights determine <strong>how strongly an input influences</strong> the output. Each weight connects a neuron in one layer to a neuron in the next layer.</p> <ul> <li>If a weight is large and positive, it strongly activates the next neuron in the next layer.</li> <li>If it’s negative, it inhibits the next neuron.</li> <li>If it’s close to zero, the input has little effect. <h3 id="what-are-biases">What Are Biases?</h3> <p>The <strong>bias</strong> is an extra parameter added to the weighted sum before applying the activation function. It allows the neuron to <strong>shift the activation function</strong>, enabling it to learn patterns that don’t pass through the origin. Without biases, the neural network’s flexibility would be significantly limited.</p> <h3 id="how-to-initialize-matrix-representation">How to Initialize: Matrix representation</h3> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/activation_matrix_form-480.webp 480w,/assets/img/blog/activation_matrix_form-800.webp 800w,/assets/img/blog/activation_matrix_form-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/activation_matrix_form.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>It is common to represent our neurons in the layers in matrix form. Let’s define the parameter shapes for our specific example:</p> <ul> <li> <strong>Input Layer → Hidden Layer</strong> <ul> <li>Weights: <code class="language-plaintext highlighter-rouge">W1</code> will be a matrix of shape <strong>(3, 4)</strong><br> (3 hidden neurons, each receiving 4 inputs)</li> <li>Biases: <code class="language-plaintext highlighter-rouge">b1</code> will be a vector of shape <strong>(3, 1)</strong><br> (1 bias for each hidden neuron)</li> </ul> </li> <li> <strong>Hidden Layer → Output Layer</strong> <ul> <li>Weights: <code class="language-plaintext highlighter-rouge">W2</code> will be a matrix of shape <strong>(1, 3)</strong><br> (1 output neuron, connected to 3 hidden neurons)</li> <li>Bias: <code class="language-plaintext highlighter-rouge">b2</code> will be a scalar or a vector of shape <strong>(1, 1)</strong><br> (bias for the output neuron) This setup assumes that we’re using <strong>column vectors</strong> for input examples (shape: 4 × 1). When initializing weights and biases, the goal is to start with small random values to break symmetry:</li> </ul> </li> <li> <strong>Weights (<code class="language-plaintext highlighter-rouge">W1</code>, <code class="language-plaintext highlighter-rouge">W2</code>)</strong>: Initialized with small random numbers (e.g., drawn from a uniform or normal distribution). This randomness helps the neurons learn different features during training. Example: <div class="language-text highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  W1 = random values of shape (3, 4)
  W2 = random values of shape (1, 3)
</code></pre></div> </div> </li> <li> <strong>Biases (<code class="language-plaintext highlighter-rouge">b1</code>, <code class="language-plaintext highlighter-rouge">b2</code>)</strong>: Usually initialized to <strong>zeros</strong>. Starting biases at zero is safe, and they’ll quickly move during training.</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/hypothesis-testing2/">Hypothesis testing- Part II- t-test</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/hypothesis-testing1/">Hypothesis testing- Part I</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/mle2/">Maximum Likelihood Estimation- part II</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/mle/">Maximum Likelihood Estimation- part I</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/bayes-theorem/">Exploring Prior, Likelihood, and Posterior Distributions</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Krishnadas Mohandas. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"For a brief overview of my professional background, see the description below. To view my full CV, click the PDF button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"dropdown-publications",title:"publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-understanding-neural-networks-part-i",title:"Understanding neural networks- Part I",description:"Defining the network architecture and intializing the NN parameters",section:"Posts",handler:()=>{window.location.href="/blog/2025/neural_netowrks_1/"}},{id:"post-hypothesis-testing-part-ii-t-test",title:"Hypothesis testing- Part II- t-test",description:"two-sample t-test for hypothesis testing",section:"Posts",handler:()=>{window.location.href="/blog/2024/hypothesis-testing2/"}},{id:"post-hypothesis-testing-part-i",title:"Hypothesis testing- Part I",description:"Introduction to hypothesis testing",section:"Posts",handler:()=>{window.location.href="/blog/2024/hypothesis-testing1/"}},{id:"post-maximum-likelihood-estimation-part-ii",title:"Maximum Likelihood Estimation- part II",description:"MLE for continuous features",section:"Posts",handler:()=>{window.location.href="/blog/2024/mle2/"}},{id:"post-maximum-likelihood-estimation-part-i",title:"Maximum Likelihood Estimation- part I",description:"MLE for discrete features",section:"Posts",handler:()=>{window.location.href="/blog/2024/mle/"}},{id:"post-exploring-prior-likelihood-and-posterior-distributions",title:"Exploring Prior, Likelihood, and Posterior Distributions",description:"Understanding the basics of Bayes' theorem",section:"Posts",handler:()=>{window.location.href="/blog/2024/bayes-theorem/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-alphorn",title:"ALPHORN",description:"Polarized relations and structural equilibrium in complex systems",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-stock-prediction-models",title:"Stock prediction models",description:"Exploring different models to predict stock price",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-customer-churning-model",title:"Customer churning model",description:"predicting customer churn in banking using ML techniques",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"projects-signed-networks",title:"Signed networks",description:"Interdisciplinary research on signed networks",section:"Projects",handler:()=>{window.location.href="/projects/signed_networks/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6B%72%69%73%68%6E%61%64%61%73%6D%39%36@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=9mbl0s0AAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/krishna-das-m","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/krishnadasm96","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>