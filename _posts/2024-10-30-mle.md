---
layout: post
title: Maximum Likelihood Estimation- part I
date: 2024-10-20 15:09:00
description: a practical example of MLE
tags: statistics code
categories: ML-concepts
featured: true
# pretty_table: true
---

In this post, we’ll dive into the concept of **Maximum Likelihood Estimation (MLE)**, particularly focusing on features that are discrete in nature. To make the discussion practical and relatable, we’ll consider the example of disease testing, building on the ideas discussed in my previous post, Exploring Prior, Likelihood, and Posterior Distributions. First, let's get familiar with the intuition behind MLE.

### What is Maximum Likelihood Estimation (MLE)?

Maximum Likelihood Estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution. It does so by maximizing the **likelihood function**, which represents the probability of the observed data given specific parameters. In simpler terms, MLE helps us find the “best fit” parameters of a model based on the data we have.

- **Likelihood**: The likelihood refers to the probability of observing the data for a given set of parameters.
- **MLE**: MLE finds the parameter values that make the observed data most probable by maximizing the likelihood function.

### Example: Estimating the Probability that a Person Tests Positive for a Disease

In our example, let’s say we want to estimate the probability that a randomly selected person tests positive for a certain disease. Since the test result is binary (either positive or negative), this is a classic case of a **Bernoulli** (or binomial) distribution problem.

Let’s set it up mathematically.

Suppose we have a random sample $$X_1, X_2, \dots, X_n$$, where:
- $$X_i = 0$$ if a randomly selected person tests **negative** for the disease.
- $$X_i = 1$$ if a randomly selected person tests **positive** for the disease.

Assuming that each test result is independent of the others (i.e., the $$X_i$$ 's are independent Bernoulli random variables), we aim to estimate the parameter $$p$$ , the probability that a person tests positive for the disease.

#### The Likelihood Function

For $$n$$ independent observations, the **likelihood function** is the joint probability of observing the data given the parameter $$p$$. This can be written as:

\begin{equation}
L(p) = \prod_{i=1}^n P(X_i | p)
\end{equation}

For a Bernoulli distribution, the probability density function for each $$X_i$$ is:

\begin{equation}
P(X_i = 1 | p) = p \quad \text{and} \quad P(X_i = 0 | p) = (1 - p)
\end{equation}

Thus, the likelihood function for the entire dataset is:

\begin{equation}
L(p) = \prod_{i=1}^n p^{X_i} (1 - p)^{1 - X_i}
\end{equation}

This represents the probability of observing the given test results (both positive and negative) for a specific value of $$p$$.

#### Example: Estimating Disease Probability Using Python

Let’s assume we don’t know the exact probability $$p$$ of having the disease. Instead, we’ll consider a range of possible values for $$p$$ (denoted as $$\theta$$) and calculate the likelihood for each value.

```python
N = 100 #total trials
nDisease = int(N*0.2) # success
# Create a list with Disease (D) and Healthy (H) results
trials = [1] * nDisease + [0] * (N - nDisease)
# Shuffle the sequence to randomize the order of trials
random.shuffle(trials)
nDisease = sum(list(map(lambda x: x==1, trials)))
nHealthy = sum(list(map(lambda x: x==0, trials)))
print(f'Number person with positive test result is: {nDisease}')
print(f'Number person with negative test result is: {nHealthy}')
```

Here’s how we generate a range of values for \( \theta \) and compute the likelihood:

```python
import numpy as np
from scipy.special import comb
import matplotlib.pyplot as plt

# Generate a range of potential probabilities for the disease
start, stop, N = 0.0, 1.0, 10
theta = np.linspace(start, stop, N)

# Calculate the likelihood function
PPositiveGiventheta = comb(N, nDisease) * theta**nDisease * (1 - theta)**nHealthy

# maximum likelihood estimation (theoretical)
MLE = (nDisease)/N

# define log-likelihood
sum_trials = np.sum(trials)
sum_one_minus_trials = np.sum([(1- x) for x in trials  ])
log_lik = sum_trials*np.log(theta) + sum_one_minus_trials*np.log(1-theta)

# Plot the likelihood function
plt.plot(theta, PPositiveGiventheta, label='Likelihood')
plt.xlabel('Theta (p)')
plt.ylabel('Likelihood')
plt.title('Likelihood Function for Disease Testing')
plt.legend()
plt.show()
```
