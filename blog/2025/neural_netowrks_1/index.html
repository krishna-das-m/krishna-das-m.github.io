<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding neural networks- Part I | Krishnadas Mohandas </title> <meta name="author" content="Krishnadas Mohandas"> <meta name="description" content="Defining the network architecture and intializing the NN parameters"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://krishna-das-m.github.io/blog/2025/neural_netowrks_1/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Krishnadas</span> Mohandas </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding neural networks- Part I</h1> <p class="post-meta"> Created in May 18, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   ·   <a href="/blog/category/ml-concepts"> <i class="fa-solid fa-tag fa-sm"></i> ML-concepts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Before diving into code, it’s essential to understand how a neural network is structured. At its core, a neural network is a system inspired by the human brain, consisting of layers of interconnected “neurons” that process data. This will be a three-part series describing each steps to understand a feedforward neural network. We’ll go through each steps below to successfully understand the basics of a neural network</p> <ol> <li>Defining neural network structure</li> <li>Initialize model parameters</li> <li>Forward Propagation</li> <li>Compute cost</li> <li>Back propagation</li> <li>Update parameters</li> <li>Training the model</li> <li>Predicting the model</li> </ol> <p>In this post we’ll cover the first two steps-designing our neural network and it’s initial state.</p> <h2 id="step-1-defining-the-neural-network-structure-understanding-layers">Step 1: Defining the Neural Network Structure: Understanding Layers</h2> <h3 id="1-input-layer">1. <strong>Input Layer</strong> </h3> <p>The input layer is the first layer of the network and serves as the gateway through which raw data enters the model. Each neuron in this layer represents one feature or attribute of the input data.</p> <p>For example, if you’re working with a dataset of handwritten digits (like the popular MNIST dataset), each image is typically 28×28 pixels, resulting in 784 input features (since 28 × 28 = 784). So, your input layer will have 784 neurons, each one receiving the pixel value of the corresponding location in the image.</p> <h3 id="2-hidden-layers">2. <strong>Hidden Layers</strong> </h3> <p>Hidden layers are the intermediate layers between the input and output layers. These layers are where most of the computation happens. Each neuron in a hidden layer takes inputs from all the neurons in the previous layer, applies a weighted sum, adds a bias, and then passes the result through an <strong>activation function</strong> (like ReLU or sigmoid) to introduce non-linearity.</p> <p>The number of hidden layers and the number of neurons in each is up to you. A single hidden layer can work for many simple tasks, but more complex problems often benefit from deeper networks with multiple hidden layers.</p> <h3 id="3-output-layer">3. <strong>Output Layer</strong> </h3> <p>The output layer is the final layer of the network, and its structure depends on the task you’re solving:</p> <ul> <li>For <strong>classification</strong>, the number of neurons equals the number of classes. For example, classifying digits 0–9 means having 10 output neurons, each one representing the probability of a particular digit.</li> <li>For <strong>regression</strong>, typically there’s just one output neuron, giving a continuous numerical value.</li> </ul> <h3 id="example-well-use-a-simple-binary-classifier">Example We’ll Use: A Simple Binary Classifier</h3> <p>Let’s say we want to build a neural network that predicts whether a customer will make a purchase based on four input features. Let’s define a few training examples to show what the network will learn from. Each training example consists of 4 input values and 1 target output. In practice, these inputs would be normalized to keep the values between 0 and 1 for better learning performance. Let’s look at a sample of what this training data might include:</p> <table> <thead> <tr> <th>Age</th> <th>Income</th> <th>Browsing Time</th> <th>Items Viewed</th> <th>Purchase?</th> </tr> </thead> <tbody> <tr> <td>0.2</td> <td>0.8</td> <td>0.5</td> <td>0.6</td> <td>1</td> </tr> <tr> <td>0.6</td> <td>0.3</td> <td>0.2</td> <td>0.4</td> <td>0</td> </tr> <tr> <td>0.3</td> <td>0.5</td> <td>0.7</td> <td>0.8</td> <td>1</td> </tr> <tr> <td>0.9</td> <td>0.2</td> <td>0.1</td> <td>0.2</td> <td>0</td> </tr> </tbody> </table> <ol> <li> <strong>Age</strong> (normalized between 0 and 1)</li> <li> <strong>Annual Income</strong> (normalized)</li> <li> <strong>Browsing Time on Website</strong> (in minutes, normalized)</li> <li> <strong>Number of Items Viewed</strong> (normalized)</li> </ol> <p>The target output is straightforward: 1 indicates a likely purchase, while 0 predicts no purchase. This normalized format optimizes the learning process and helps our network identify patterns more efficiently. Notice how examples with higher browsing time and more items viewed tend to result in purchases - these are exactly the kinds of patterns our neural network will learn to recognize.</p> <h3 id="network-architecture">Network Architecture</h3> <p>For our customer purchase prediction model, we’ll implement a straightforward 2 layer neural network architecture:</p> <ul> <li> <strong>Input Layer</strong>: 4 neurons (each corresponding to one input feature)</li> <li> <strong>Hidden Layer</strong>: 3 neurons (fully connected to all input neurons)</li> <li> <strong>Output Layer</strong>: 1 neuron (outputs a value between 0 and 1 — interpreted as purchase probability)</li> </ul> <p>This architecture is technically considered a two-layer network in modern deep learning terminology, as the input layer doesn’t perform calculations but simply passes data forward. A schematic representation of the NN is shown below.</p> <div class="row justify-content-center"> <div class="col-sm-8 col-md-6 mt-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/neural_network-480.webp 480w,/assets/img/blog/neural_network-800.webp 800w,/assets/img/blog/neural_network-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/neural_network.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>When examining training data, we denote the \(m^{th}\) training example for the \(i^{th}\) input feature as \(x_i^{(m)}\).For neuron activations, we use \(a_i^{[l]}\) to represent the output value of the \(i^{th}\) neuron in layer \(l\) after applying the activation function. More generally, we can represent a neuron in any adjacent preceding layer as \(a_i^{[l-1]}\).</p> <p>Great! Now that we’ve defined the structure of the neural network, the next step is to <strong>initialize the parameters</strong> — specifically, the <strong>weights</strong> and <strong>biases</strong>. These parameters are what the network will learn during training.</p> <h2 id="step-2-initializing-parameters--weights-and-biases">Step 2: Initializing Parameters – Weights and Biases</h2> <p>Every connection between neurons in adjacent layers has a <strong>weight</strong>, and every neuron (except those in the input layer) has an associated <strong>bias</strong>. These are the values that the network updates during training to reduce the prediction error.</p> <h3 id="what-are-weights">What Are Weights?</h3> <p>Weights determine <strong>how strongly an input influences</strong> the output. Each weight connects a neuron in one layer to a neuron in the next layer.</p> <ul> <li>If a weight is large and positive, it strongly activates the next neuron in the next layer.</li> <li>If it’s negative, it inhibits the next neuron.</li> <li>If it’s close to zero, the input has little effect.</li> </ul> <h3 id="what-are-biases">What Are Biases?</h3> <p>The <strong>bias</strong> is an extra parameter added to the weighted sum before applying the activation function. It allows the neuron to <strong>shift the activation function</strong>, enabling it to learn patterns that don’t pass through the origin. Without biases, the neural network’s flexibility would be significantly limited.</p> <h3 id="how-to-initialize-matrix-representation">How to Initialize: Matrix representation</h3> <p>Let’s start with a visual. Imagine our neural network with an input layer and a hidden layer, as shown in the diagram.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/activation_matrix_form-480.webp 480w,/assets/img/blog/activation_matrix_form-800.webp 800w,/assets/img/blog/activation_matrix_form-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/activation_matrix_form.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Consider the connections between our input and hidden layers. While a single input observation might be shown in a schematic, thinking about the entire training data in the input layer is essential for understanding how matrix operations power neural networks.</p> <p>At the core of every neural network connection are <strong>weights</strong>, learnable parameters that define the strength of the connection between neurons. Alongside weights, <strong>biases</strong> provide an extra degree of freedom for the network to learn. To make the connections clearer, we’ve color-coded the weights, biases, and the resulting neuron values in the hidden layer in our diagram.</p> <p>To clarify the upcoming calculations, let’s refine our notation For a given layer \(l\), the weights connecting to neuron \(i\) are represented as a vector \(w_i^{[l]}\)​. The subscript \(i\) indicates the neuron receiving the weights, and the superscript \(l\) indicates the layer. This is shown in the figure below. Similarly, the activation of neuron \(i\) in layer \(l\) is represented as \(a_i^{[l]}\). For example, \(a_1^{[1]}\)​ represents the activation of the first neuron in the first hidden layer.</p> <div class="row justify-content-center"> <div class="col-sm-8 col-md-6 mt-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/weight_matrix-480.webp 480w,/assets/img/blog/weight_matrix-800.webp 800w,/assets/img/blog/weight_matrix-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/weight_matrix.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>For a single neuron, say the first neuron in the first hidden layer \((i=1,l=1)\), the input to this neuron before the activation function, often denoted as \(z\), is calculated as the weighted sum of the activations from the preceding layer (\(a[0]\) represents the input layer activations) plus the bias term (\(b_1^{[1]}\)​):</p> \[z_1^{[1]}​=w_1^{[1]T}​a^{[0]}+b_1^{[1]}\] <p>This value \(z_1^{[1]}​\) is then passed through the activation function, \(g(z)\), to get the activation of the neuron:</p> \[a_1^{[1]}​=g(z_1^{[1]}​)\] <p>More generally, for neuron \(i\) in layer \(l\), the calculation is:</p> \[z_i^{[l]}​=w_i^{[l]T}​a^{[l−1]}+b_i^{[l]}​\] \[a_i^{[l]}​=g(z_i{[l]}​)\] <p>While understanding individual neuron calculations is important, neural networks primarily leverage linear algebra to perform these computations for an entire layer simultaneously. This is where the matrix representation becomes invaluable.</p> <p>We can calculate the \(Z\) values for all neurons in layer \(l\) using the weight matrix \(W^{[l]}\) (where each row is a weight vector \(w_i^{[l]T}​\), the activation matrix from the previous layer \(A^{[l−1]}\), and the bias vector \(b^{[l]}\):</p> \[Z^{[l]}=W^{[l]}A^{[l−1]}+b^{[l]}\] <p>Notice the difference in how the weight matrix \(W^{[l]}\) is used here compared to the single neuron equation. In the matrix equation, \(W^{[l]}\) is structured so that when multiplied by the activation matrix \(A^{[l−1]}\), it simultaneously computes the weighted sum (\(w_i^{[l]T}​a^{[l−1]})\) for every neuron \(i\) in layer \(l\) across all training examples. This single matrix multiplication efficiently performs all the necessary dot products at once!</p> <p>Finally, applying the activation function g element-wise to the matrix \(Z^{[l]}\) gives us the activation matrix for layer \(l\), \(A^{[l]}\):</p> \[A^{[l]}=g(Z^{[l]})\] <p>This matrix \(A^{[l]}\) then serves as the input activation for the next layer’s calculations. This matrix-based approach allows for highly parallelized and efficient computation, which is fundamental to training neural networks on large datasets.</p> <h3 id="understanding-the-parameter-shapes">Understanding the Parameter Shapes</h3> <p>The dimensions of these matrices and vectors are crucial for ensuring the math works out. Let’s define the shapes for our specific example network (with 4 input neurons, 3 hidden neurons, and 1 output neuron) and then generalize:</p> <ul> <li> <strong>Input Layer</strong> → <strong>Hidden Layer (Layer 1):</strong> <ul> <li> <strong>Weights (</strong>\(W^{[1]}\)<strong>):</strong> This matrix contains the weights connecting the 4 input neurons to the 3 hidden neurons. Its shape is <strong>(3, 4)</strong>.</li> <li> <strong>Biases</strong> (\(b^{[1]}\)): There is one bias term for each of the 3 hidden neurons. This is represented as a vector of shape <strong>(3, 1)</strong>.</li> </ul> </li> <li> <strong>Hidden Layer</strong> → <strong>Output Layer (Layer 2):</strong> <ul> <li> <strong>Weights (</strong>\(W^{[2]}\)<strong>):</strong> This matrix connects the 3 hidden neurons to the single output neuron. Its shape is <strong>(1, 3)</strong>.</li> <li> <strong>Bias</strong> (\(b^{[2]}\)): There is one bias term for the single output neuron. This is represented as a vector of shape <strong>(1, 1)</strong> (or simply a scalar).</li> </ul> </li> </ul> <p>Generally, the weight matrix \(W^{[1]}\) has the shape (\(n_h​^{[l]}×n_h^{[l-1]}​\)), where \(n_h^{[l]}\) is the number of hidden units in layer \(l\) and the bias vector \(b^{[1]}\) has the shape (\(n_h^{[l]}​×1\)). The activations of a given layer will be a matrix of shape \((n_h^{[l]}\times m)\) where \(m\) represents the number of observations being fed through the network.</p> <p>So, the next time you encounter a neural network, remember the elegant matrix operations happening under the hood. This fundamental understanding is your key to building and training more complex models!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/hypothesis-testing2/">Hypothesis testing- Part II- t-test</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/hypothesis-testing1/">Hypothesis testing- Part I</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/mle2/">Maximum Likelihood Estimation- part II</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/mle/">Maximum Likelihood Estimation- part I</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/bayes-theorem/">Exploring Prior, Likelihood, and Posterior Distributions</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Krishnadas Mohandas. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"For a brief overview of my professional background, see the description below. To view my full CV, click the PDF button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"dropdown-publications",title:"publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-understanding-neural-networks-part-i",title:"Understanding neural networks- Part I",description:"Defining the network architecture and intializing the NN parameters",section:"Posts",handler:()=>{window.location.href="/blog/2025/neural_netowrks_1/"}},{id:"post-hypothesis-testing-part-ii-t-test",title:"Hypothesis testing- Part II- t-test",description:"two-sample t-test for hypothesis testing",section:"Posts",handler:()=>{window.location.href="/blog/2024/hypothesis-testing2/"}},{id:"post-hypothesis-testing-part-i",title:"Hypothesis testing- Part I",description:"Introduction to hypothesis testing",section:"Posts",handler:()=>{window.location.href="/blog/2024/hypothesis-testing1/"}},{id:"post-maximum-likelihood-estimation-part-ii",title:"Maximum Likelihood Estimation- part II",description:"MLE for continuous features",section:"Posts",handler:()=>{window.location.href="/blog/2024/mle2/"}},{id:"post-maximum-likelihood-estimation-part-i",title:"Maximum Likelihood Estimation- part I",description:"MLE for discrete features",section:"Posts",handler:()=>{window.location.href="/blog/2024/mle/"}},{id:"post-exploring-prior-likelihood-and-posterior-distributions",title:"Exploring Prior, Likelihood, and Posterior Distributions",description:"Understanding the basics of Bayes' theorem",section:"Posts",handler:()=>{window.location.href="/blog/2024/bayes-theorem/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-alphorn",title:"ALPHORN",description:"Polarized relations and structural equilibrium in complex systems",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-stock-prediction-models",title:"Stock prediction models",description:"Exploring different models to predict stock price",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-customer-churning-model",title:"Customer churning model",description:"predicting customer churn in banking using ML techniques",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"projects-signed-networks",title:"Signed networks",description:"Interdisciplinary research on signed networks",section:"Projects",handler:()=>{window.location.href="/projects/signed_networks/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6B%72%69%73%68%6E%61%64%61%73%6D%39%36@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=9mbl0s0AAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/krishna-das-m","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/krishnadasm96","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>