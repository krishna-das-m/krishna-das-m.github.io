<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Maximum Likelihood Estimation- part I | Krishnadas Mohandas </title> <meta name="author" content="Krishnadas Mohandas"> <meta name="description" content="a practical example of MLE"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://krishna-das-m.github.io/blog/2024/mle/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Krishnadas</span> Mohandas </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Maximum Likelihood Estimation- part I</h1> <p class="post-meta"> Created in October 25, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> statistics</a>   <a href="/blog/tag/code"> <i class="fa-solid fa-hashtag fa-sm"></i> code</a>   ·   <a href="/blog/category/ml-concepts"> <i class="fa-solid fa-tag fa-sm"></i> ML-concepts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In this post, we’ll dive into the concept of <strong>Maximum Likelihood Estimation (MLE)</strong>, particularly focusing on features that are discrete in nature. To make the discussion practical and relatable, we’ll consider the example of disease testing, building on the ideas discussed in my previous post, Exploring Prior, Likelihood, and Posterior Distributions. First, let’s get familiar with the intuition behind MLE.</p> <h3 id="what-is-maximum-likelihood-estimation-mle">What is Maximum Likelihood Estimation (MLE)?</h3> <p>Maximum Likelihood Estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution. It does so by maximizing the <strong>likelihood function</strong>, which represents the probability of the observed data given specific parameters. In simpler terms, MLE helps us find the “best fit” parameters of a model based on the data we have.</p> <ul> <li> <strong>Likelihood</strong>: The likelihood refers to the probability of observing the data for a given set of parameters.</li> <li> <strong>MLE</strong>: MLE finds the parameter values that make the observed data most probable by maximizing the likelihood function.</li> </ul> <h3 id="example-estimating-the-probability-that-a-person-tests-positive-for-a-disease">Example: Estimating the Probability that a Person Tests Positive for a Disease</h3> <p>In our example, let’s say we want to estimate the probability that a randomly selected person tests positive for a certain disease. Since the test result is binary (either positive or negative), this is a classic case of a <strong>Bernoulli</strong> (or binomial) distribution problem.</p> <p>Let’s set it up mathematically.</p> <p>Suppose we have a random sample \(X_1, X_2, \dots, X_n\), where:</p> <ul> <li>\(X_i = 0\) if a randomly selected person tests <strong>negative</strong> for the disease.</li> <li>\(X_i = 1\) if a randomly selected person tests <strong>positive</strong> for the disease.</li> </ul> <p>Assuming that each test result is independent of the others (i.e., the \(X_i\) ‘s are independent Bernoulli random variables), we aim to estimate the parameter \(p\) , the probability that a person tests positive for the disease.</p> <h4 id="the-likelihood-function">The Likelihood Function</h4> <p>For \(n\) independent observations, the <strong>likelihood function</strong> is the joint probability of observing the data given the parameter \(p\). This can be written as:</p> <p>\begin{equation} L(p) = \prod_{i=1}^n P(X_i | p) \end{equation}</p> <p>For a Bernoulli distribution, the probability density function for each \(X_i\) is:</p> <p>\begin{equation} P(X_i = 1 | p) = p \quad \text{and} \quad P(X_i = 0 | p) = (1 - p) \end{equation}</p> <p>Thus, the likelihood function for the entire dataset is:</p> <p>\begin{equation} L(p) = \prod_{i=1}^n p^{X_i} (1 - p)^{1 - X_i} \end{equation}</p> <p>This represents the probability of observing the given test results (both positive and negative) for a specific value of \(p\).</p> <p>Our goal here is to estimate \(p\), which is the probability that a random individual will test positive for the disease. Let’s create a random sample as described mathematically above. We’ll also look at how sample size effects the likelihood.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">scipy.special</span> <span class="kn">import</span> <span class="n">comb</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="k">def</span> <span class="nf">samples</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">percent</span><span class="p">):</span>
<span class="err"> </span> <span class="err"> </span> <span class="n">k</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">N</span><span class="o">*</span><span class="n">percent</span><span class="p">)</span> <span class="c1"># success
</span><span class="err"> </span> <span class="err"> </span> <span class="n">trials</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="n">k</span><span class="p">)</span>
<span class="err"> </span> <span class="err"> </span> <span class="k">return</span> <span class="n">trials</span><span class="p">,</span> <span class="n">k</span>

<span class="n">trials_1</span><span class="p">,</span> <span class="n">k_1</span> <span class="o">=</span> <span class="nf">samples</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span><span class="n">percent</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">trials_2</span><span class="p">,</span> <span class="n">k_2</span> <span class="o">=</span> <span class="nf">samples</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span><span class="n">percent</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">trials_3</span><span class="p">,</span> <span class="n">k_3</span> <span class="o">=</span> <span class="nf">samples</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">percent</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</code></pre></div></div> <p>We have created three random samples with same percentage of people tested positive for the disease (for comparison). Let’s assume we don’t know the exact probability \(p\) (we know it 0.2) of having the disease. Instead, we’ll consider a range of possible values for \(p\) and calculate the likelihood for each value. Here’s how we generate a range of values for \(p\) and compute the likelihood:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># probability of sucess
</span><span class="k">def</span> <span class="nf">success_prob</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
<span class="err"> </span> <span class="err"> </span> <span class="n">start</span><span class="p">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span><span class="mf">1.0</span>
<span class="err"> </span> <span class="err"> </span> <span class="n">h</span> <span class="o">=</span> <span class="p">(</span><span class="n">stop</span><span class="o">-</span><span class="n">start</span><span class="p">)</span><span class="o">/</span><span class="n">N</span>
<span class="err"> </span> <span class="err"> </span> <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
<span class="err"> </span> <span class="err"> </span> <span class="k">return</span> <span class="n">p</span>

<span class="n">p_1</span> <span class="o">=</span> <span class="nf">success_prob</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">p_2</span> <span class="o">=</span> <span class="nf">success_prob</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">p_3</span> <span class="o">=</span> <span class="nf">success_prob</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># Calculate the likelihood function
</span><span class="k">def</span> <span class="nf">likelihood</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
<span class="err"> </span> <span class="err"> </span> <span class="k">return</span> <span class="nf">comb</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span><span class="o">*</span> <span class="n">p</span><span class="o">**</span><span class="n">k</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="n">N</span><span class="o">-</span><span class="n">k</span><span class="p">)</span>

<span class="n">likelihood_p_1</span> <span class="o">=</span> <span class="nf">likelihood</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">k_1</span><span class="p">,</span> <span class="n">p_1</span><span class="p">)</span> <span class="c1"># Likelihood
</span><span class="n">likelihood_p_2</span> <span class="o">=</span> <span class="nf">likelihood</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">k_2</span><span class="p">,</span> <span class="n">p_2</span><span class="p">)</span>
<span class="n">likelihood_p_3</span> <span class="o">=</span> <span class="nf">likelihood</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">k_3</span><span class="p">,</span> <span class="n">p_3</span><span class="p">)</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LikelihoodFunction-480.webp 480w,/assets/img/LikelihoodFunction-800.webp 800w,/assets/img/LikelihoodFunction-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/LikelihoodFunction.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>For small sample sizes (\(N=20\)), the likelihood curve is broad. This means a wide range of \(p\) values seem plausible — the data doesn’t provide much information to narrow down the estimate of \(p\). As the sample size increases (moving to \(N=50\) and \(N=100\)), the curve becomes narrower. This reflects that larger datasets provide more information, allowing us to pinpoint the most likely value of \(p\) with greater precision. The peak of each curve represents the <strong>maximum likelihood estimate (MLE)</strong>, the value of \(p\) that maximizes the likelihood function.</p> <p>Maximizing the <strong>likelihood</strong> to find MLE, directly can be cumbersome due to the product of many terms. However, we can simplify the process by taking the <strong>logarithm</strong> of the likelihood function. Since the natural logarithm is a monotonic function, maximizing the log-likelihood is equivalent to maximizing the likelihood itself. The log-likelihood function is:</p> <p>\begin{equation} \ell(p) = \log L(p) = \log \left( \prod_{i=1}^n p^{X_i} (1 - p)^{1 - X_i} \right) \end{equation}</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define log-likelihood
</span><span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">trials</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
<span class="err"> </span> <span class="err"> </span> <span class="n">sum_trials</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">trials</span><span class="p">)</span>
<span class="err"> </span> <span class="err"> </span> <span class="n">sum_one_minus_trials</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">([(</span><span class="mi">1</span><span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">trials</span> <span class="err"> </span><span class="p">])</span>
<span class="err"> </span> <span class="err"> </span> <span class="n">log_lik</span> <span class="o">=</span> <span class="n">sum_trials</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="n">sum_one_minus_trials</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span>
<span class="err"> </span> <span class="err"> </span> <span class="n">mle</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">log_lik</span><span class="p">)]</span>
<span class="err"> </span> <span class="err"> </span> <span class="k">return</span> <span class="n">log_lik</span><span class="p">,</span> <span class="n">mle</span>

<span class="n">log_lik_1</span><span class="p">,</span> <span class="n">mle_1</span> <span class="o">=</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">trials_1</span><span class="p">,</span><span class="n">p_1</span><span class="p">)</span>
<span class="n">log_lik_2</span><span class="p">,</span> <span class="n">mle_2</span> <span class="o">=</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">trials_2</span><span class="p">,</span><span class="n">p_2</span><span class="p">)</span>
<span class="n">log_lik_3</span><span class="p">,</span> <span class="n">mle_3</span> <span class="o">=</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">trials_3</span><span class="p">,</span><span class="n">p_3</span><span class="p">)</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LogLikelihood-480.webp 480w,/assets/img/LogLikelihood-800.webp 800w,/assets/img/LogLikelihood-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/LogLikelihood.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Just like the likelihood, the log-likelihood curve peaks at the MLE, which is indicated by the dashed red line in the plot. Notice how, for larger sample sizes, the log-likelihood curve becomes steeper around the MLE. This means that the likelihood sharply penalizes values of \(p\) that deviate from the MLE as we gather more data. The shape of the curves is consistent across sample sizes, but the curves shift downwards as \(N\) increases. This is a natural consequence of the log-likelihood being the sum of the log probabilities across all data points — with more data, this sum grows larger (in negative magnitude).<br> The MLE turns out to be 0.2 which is nothing but the proportion of people who tested positive in the given sample. This is can be written as</p> <p>\begin{equation} \hat{p} = \frac{\sum_{i=1}^n X_i}{n} \end{equation}</p> <p>This tells us that the maximum likelihood estimate \(\hat{p}\) is simply the <strong>sample mean</strong> of the observed data.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/bayes-theorem/">Exploring Prior, Likelihood, and Posterior Distributions</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Krishnadas Mohandas. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"For a brief overview of my professional background, see the description below. To view my full CV, click the PDF button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"dropdown-publications",title:"publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-maximum-likelihood-estimation-part-i",title:"Maximum Likelihood Estimation- part I",description:"a practical example of MLE",section:"Posts",handler:()=>{window.location.href="/blog/2024/mle/"}},{id:"post-exploring-prior-likelihood-and-posterior-distributions",title:"Exploring Prior, Likelihood, and Posterior Distributions",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2024/bayes-theorem/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-alphorn",title:"ALPHORN",description:"Polarized relations and structural equilibrium in complex systems",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-customer-churning-model",title:"Customer churning model",description:"predicting customer churn in banking using ML techniques",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6B%72%69%73%68%6E%61%64%61%73%6D%39%36@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=9mbl0s0AAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/krishna-das-m","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/krishnadasm96","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>