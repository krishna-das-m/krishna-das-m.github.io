<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://krishna-das-m.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://krishna-das-m.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-12T10:18:44+00:00</updated><id>https://krishna-das-m.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Exploring Prior, Likelihood, and Posterior Distributions</title><link href="https://krishna-das-m.github.io/blog/2024/bayes-theorem/" rel="alternate" type="text/html" title="Exploring Prior, Likelihood, and Posterior Distributions"/><published>2024-09-30T15:09:00+00:00</published><updated>2024-09-30T15:09:00+00:00</updated><id>https://krishna-das-m.github.io/blog/2024/bayes-theorem</id><content type="html" xml:base="https://krishna-das-m.github.io/blog/2024/bayes-theorem/"><![CDATA[<p>In this post, we’ll explore the relationship between <strong>prior distribution</strong>, <strong>likelihood</strong>, and <strong>posterior distribution</strong> using a practical example.</p> <h4 id="bayes-theorem">Bayes’ Theorem</h4> <p>Bayes’ Theorem helps us update our beliefs based on new evidence. Here’s the formula: \begin{equation} \overbrace{P(hypothesis | evidence)}^{\text{posterior}} = \frac{\overbrace{P(evidence| hypothesis)}^{\text{likelihood}} \overbrace{P(hypothesis)}^{\text{prior}}}{\underbrace{P(evidence)}_{\text{marginal}}} \end{equation}</p> <p>Where:</p> <ul> <li><strong>Posterior</strong>: How probable is our hypothesis given the observed evidence.</li> <li><strong>Likelihood</strong>: How probable is the evidence given that our hypothesis is true.</li> <li><strong>Prior</strong>: How probable was our hypothesis before observing the data.</li> <li><strong>Marginal</strong>: How probable is the new evidence under all possible hypothesis.</li> </ul> <hr/> <h4 id="the-scenario">The Scenario</h4> <p>Let’s walk through an example.</p> <p>Suppose you visit your doctor and get tested for a rare disease. Unfortunately, the test result is positive. Naturally, you want to know: “Given the positive result, what is the probability that I actually have the disease?” Since no medical test is 100% accurate, Bayes’ Theorem helps us answer this question. In the context of our current scenario, when comparing it to the general form of Bayes’ Theorem, the hypothesis we are testing is that a person has the disease (i.e., they test positive for it). The evidence corresponds to the observed data, specifically the results from the tests that indicate a positive outcome.</p> <hr/> <h4 id="breaking-down-the-terms">Breaking Down the Terms:</h4> <ol> <li> <p><strong>Prior Probability, \(P(Disease)\)</strong>:<br/> This is the probability of having the disease before considering the test result. It represents the incidence of the disease in the general population.<br/> For example, if the disease affects 1 in 1000 people, then:<br/> \(P(Disease) = 0.001\)</p> </li> <li> <p><strong>Likelihood, \(P(+ | Disease)\)</strong>:<br/> This is the probability of testing positive, given that you already have the disease. In other words, it’s the accuracy of the test in detecting the disease when it is present.<br/> Let’s assume the test has an accuracy of 99%, so:<br/> \(P(+ | Disease) = 0.99\)</p> </li> <li> <p><strong>Marginal, \(P(+)\)</strong>:<br/> This is the overall probability of testing positive, regardless of whether the person has the disease. It includes both true positives and false positives.<br/> It is calculated as:<br/> \(P(+) = P(Disease) P(+ | Disease) + P(Healthy) P(+ | Healthy)\)<br/> Here, \(P(Healthy) = 1 - P(Disease) = 0.999\), and \(P(+ | Healthy)\) is the probability of NOT having the disease and falsely identified positive by the test (1% = 0.01). So:<br/> \(P(+ | Healthy) = 0.01\)</p> </li> <li> <p><strong>Posterior Probability, \(P(Disease | +)\)</strong>:<br/> This is the updated probability of having the disease, given that the test result is positive. Using Bayes’ Theorem, we can compute this as:<br/> \(P(Disease | +) = \frac{P(Disease) P(+ | Disease)}{P(+) }\) Plugging in the values:<br/> \(P(Disease | +) = \frac{0.001 \times 0.99}{(0.001 \times 0.99) + (0.999 \times 0.01)} = 0.09016\)<br/> So, the probability that you actually have the disease, even after testing positive, is about <strong>9%</strong>.</p> </li> </ol> <hr/> <p>Now where do the distribution plots of prior, likelihood and posterior come from? Before diving into how we obtain the distributions for the <strong>prior</strong>, <strong>likelihood</strong>, and <strong>posterior</strong>, let’s introduce a change of variable to make the equation more intuitive. We’ll substitute <strong>Disease</strong> with <strong>\(\theta\)</strong>, which represents the probability that a person from the population will test positive. This allows us to express <strong>Bayes’ Theorem</strong> as:<br/> \begin{equation} P(\theta|+)=\frac{P(+|\theta)P(\theta)}{P(+)} \end{equation} Our goal is to estimate the parameter <strong>\(\theta\)</strong>, which is the probability that a random individual will test positive for the disease. To make this concept more concrete, let’s simulate a scenario where we conduct a total of 10 tests, out of which 2 people are positive (we’ll call this a <strong>success</strong>).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1"># Create bernoulli trials
</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1">#total trials
</span> <span class="n">nDisease</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># success
</span>
 <span class="c1"># Create a list with Disease (D) and Healthy (H) results
</span> <span class="n">trials</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">D</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="n">nDisease</span> <span class="o">+</span> <span class="p">[</span><span class="sh">"</span><span class="s">H</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="n">nDisease</span><span class="p">)</span>

 <span class="c1"># Shuffle the sequence to randomize the order of trials
</span> <span class="n">random</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">trials</span><span class="p">)</span>

 <span class="nf">print</span><span class="p">(</span><span class="n">trials</span><span class="p">)</span>
 <span class="p">[</span><span class="sh">'</span><span class="s">H</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">H</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">H</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">H</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">H</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">H</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">H</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">D</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">H</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">D</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div> <hr/> <h4 id="the-prior-distribution"><strong>The Prior Distribution</strong></h4> <p>Let’s assume we don’t have any specific information about the proportion of people who have the disease, so we don’t know the exact value of \(P(\theta)\). Instead of assuming a single fixed value, we’ll consider a range of possible values for \(P(\theta)\).<br/> To account for this uncertainty, we’ll generate 10 random values between 0 and 1, which will represent potential probabilities for the prevalence of the disease in the population. This helps us explore various possible scenarios when estimating the likelihood of having the disease based on a positive test result.<br/> Here’s how we generate these random values:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="c1"># Disease = np.random.random(10)
</span><span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">10</span>
<span class="n">h</span> <span class="o">=</span> <span class="p">(</span><span class="n">stop</span><span class="o">-</span><span class="n">start</span><span class="p">)</span><span class="o">/</span><span class="n">N</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
</code></pre></div></div> <p>At this stage, <code class="language-plaintext highlighter-rouge">theta</code> contains a set of values representing possible probabilities of having the disease. However, these values don’t yet form a probability distribution because the sum of \(P(\theta)\) is not equal to 1. If desired, we could adjust the range of <code class="language-plaintext highlighter-rouge">theta</code> (for example, using values from 0 to 10), but in that case, we would need to normalize the values.</p> <p>Next, we need to choose a distribution for the <strong>prior</strong>. A suitable choice is the <strong>beta distribution</strong>, which is defined over the interval \([0, 1]\), making it perfect for modeling probabilities like \(\theta\).</p> <p>We can specify the prior distribution using the beta distribution:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define prior distribution
</span><span class="n">a</span> <span class="p">,</span> <span class="n">b</span><span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span>
<span class="n">dist</span> <span class="o">=</span> <span class="nf">beta</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">Ptheta</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="c1"># Normalize so that values sum to 1
</span><span class="n">Ptheta</span> <span class="o">=</span> <span class="n">Ptheta</span> <span class="o">/</span> <span class="nf">sum</span><span class="p">(</span><span class="n">Ptheta</span><span class="p">)</span>
<span class="c1"># print(pTheta)
</span></code></pre></div></div> <p>The array <code class="language-plaintext highlighter-rouge">Ptheta</code> gives us the prior probability distribution. This represents our <strong>prior belief</strong> about the possible values of \(P(\theta)\) before observing any new data, such as the results of the test.</p> <h4 id="the-likelihood"><strong>The Likelihood</strong></h4> <p><strong>The likelihood function</strong> is simply the joint probability of observing the data we have. For a set of test results, the likelihood function is expressed as:<br/> \(P(+|\theta)=\prod_i^N P(+_i|\theta)\) This is the product of the probabilities of observing each positive test result for a given \(\theta\). Specifically, the likelihood function for this binary event (positive or negative test result) can be written using the <strong>binomial distribution</strong>:</p> <p>\(P(+|\theta)=\binom{N}{k}\theta^k(1-\theta)^{N-k}\)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Likelihood
</span><span class="n">PPositiveGiventheta</span> <span class="o">=</span> <span class="nf">comb</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">nDisease</span><span class="p">)</span><span class="o">*</span><span class="n">theta</span><span class="o">**</span><span class="n">nDisease</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span><span class="o">**</span><span class="n">nHealthy</span>
</code></pre></div></div> <h4 id="the-posterior-distribution"><strong>The Posterior Distribution</strong></h4> <p>With both the <strong>prior</strong> and <strong>likelihood</strong> in hand, we can now calculate the <strong>posterior distribution</strong> using Bayes’ Theorem. The only remaining part to compute is the denominator of Bayes’ Theorem, which acts as a normalization factor, ensuring that the posterior probabilities sum to 1.</p> <p>Using the following Python code, we can calculate the <strong>marginal probability</strong> (the denominator) and the posterior:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># marginal probability
</span><span class="n">PPositive</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">PPositiveGiventheta</span> <span class="o">*</span> <span class="n">Ptheta</span><span class="p">)</span>
<span class="c1"># posterior probability
</span><span class="n">PthetaGivenPositive</span> <span class="o">=</span> <span class="n">PPositiveGiventheta</span><span class="o">*</span><span class="n">Ptheta</span> <span class="o">/</span> <span class="n">PPositive</span>
</code></pre></div></div> <p>Now let’s take a look at what we’ve calculated for each value of \(\theta\):</p> <table> <thead> <tr> <th>theta</th> <th>prior</th> <th>likelihood</th> <th>posterior</th> </tr> </thead> <tbody> <tr> <td>0.0</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> </tr> <tr> <td>0.1</td> <td>0.0545</td> <td>0.1937</td> <td>0.1106</td> </tr> <tr> <td>0.2</td> <td>0.0970</td> <td>0.3020</td> <td>0.3065</td> </tr> <tr> <td>0.3</td> <td>0.1273</td> <td>0.2335</td> <td>0.3110</td> </tr> <tr> <td>0.4</td> <td>0.1455</td> <td>0.1209</td> <td>0.1841</td> </tr> <tr> <td>0.5</td> <td>0.1515</td> <td>0.0439</td> <td>0.0697</td> </tr> <tr> <td>0.6</td> <td>0.1455</td> <td>0.0106</td> <td>0.0162</td> </tr> <tr> <td>0.7</td> <td>0.1273</td> <td>0.0014</td> <td>0.0019</td> </tr> <tr> <td>0.8</td> <td>0.0970</td> <td>0.0001</td> <td>0.0001</td> </tr> <tr> <td>0.9</td> <td>0.0545</td> <td>0.0000</td> <td>0.0000</td> </tr> </tbody> </table> <p>Starting with the <strong>prior</strong> column, we can see that, based on the beta distribution \(\beta(2,2)\), most of the prior probability is centered around middle values of \(\theta\). This reflects an initial belief that the probability of having the disease is likely to be between 0.4 and 0.6, while assigning smaller probabilities to extreme values close to 0 or 1. The <strong>likelihood</strong>, derived from the observed data, suggests that the most likely values for \(\theta\) are between 0.2 and 0.3. Specifically, the maximum likelihood estimate for \(\theta\) is 0.2, which corresponds to the proportion of positive test results observed in the sample. As a result, the <strong>posterior distribution</strong> represents an update to our prior belief based on the likelihood of the data. The posterior shifts the bulk of the probability away from the prior’s most likely values (around 0.4–0.6) and moves it toward the data-driven estimate of \(\theta\) around 0.2. If we look at the mean of the posterior,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">posteriorMean</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">PDiseaseGivenPositive</span> <span class="o">*</span> <span class="n">theta</span><span class="p">)</span>
<span class="n">posteriorMean</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="mf">0.285</span>
</code></pre></div></div> <p>We can perhaps understand this further via the following visualizations. When using a <strong>beta prior</strong> of \(\beta(2,2)\), we start with a fairly weak belief about the probability \(\theta\) (testing positive for the disease). This prior distribution is fairly spread out, indicating that we don’t have a strong belief in any particular value of \(\theta\). There’s no sharp peak—suggesting we are open to a wide range of possibilities for the true value of \(\theta\). The <strong>likelihood function</strong>, based on the observed data, peaks around \(\theta = 0.1\), indicating that the data suggests this is the most likely value for the probability of a positive test. However, since our sample size is small, the likelihood still has some spread, reflecting uncertainty in our estimate. The <strong>posterior distribution</strong>—in green—represents the updated belief after combining the prior with the likelihood. It shows a peak around \(\theta = 0.2\), reflecting that the data is suggesting a slightly higher value for \(\theta\). However, since we’re combining a weak prior and a small sample, the posterior still has noticeable spread, indicating uncertainty remains.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/distributions_prior_beta_2_2-480.webp 480w,/assets/img/distributions_prior_beta_2_2-800.webp 800w,/assets/img/distributions_prior_beta_2_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/distributions_prior_beta_2_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/distributions_prior_beta_10_10-480.webp 480w,/assets/img/distributions_prior_beta_10_10-800.webp 800w,/assets/img/distributions_prior_beta_10_10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/distributions_prior_beta_10_10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Now, consider a stronger prior, \(\beta(10,10)\), along with a larger dataset (100 samples). The prior distribution is more concentrated, reflecting a stronger initial belief about \(\theta\)—in this case, that it’s around 0.5. The <strong>likelihood function</strong> with the larger sample is also much sharper and concentrated around \(\theta = 0.2\). This sharpness reflects the informativeness of the data: with more samples, the data has reduced uncertainty, and the peak clearly suggests that \(\theta = 0.2\) is the most likely value. Finally, the <strong>posterior distribution</strong> in this case is sharper and more concentrated around \(\theta = 0.2\), indicating much higher certainty about the estimate of \(\theta\). With the larger dataset, the prior has much less influence compared to the likelihood, which dominates and drives the sharper posterior.</p> <p>Now, let’s look what happens when we use the posterior distribution from the previous step as the new prior. This updated prior now reflects a more informed belief about the unknown parameter \(\theta\). Since this new prior is derived from the posterior of the previous round, it’s much more concentrated compared to the original prior we started with. We now have a much stronger belief that \(\theta\) lies within a narrow range (around 0.15 in this case). The likelihood function looks almost the same as in the previous figure and still shows a peak around the same value of \(\theta\), indicating that the observed data strongly suggests that \(\theta\) falls within a same range. The updated posterior distribution is even sharper and more concentrated than before, reflecting a very strong belief about the value of \(\theta\). Compared to the earlier posterior (based on a broader prior), this one is peaked around 0.12, showing that we now have an even greater degree of certainty about \(\theta\).<br/> The key takeaway here is that as we gather more data and update our beliefs (using Bayes’ Theorem), our estimates become more precise. The prior becomes more informative, and the posterior narrows further, reflecting reduced uncertainty about the unknown parameter .</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/distributions_prior_posterior-480.webp 480w,/assets/img/distributions_prior_posterior-800.webp 800w,/assets/img/distributions_prior_posterior-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/distributions_prior_posterior.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Bayesian updating is a powerful method that allows us to continuously refine our estimates as new data becomes available. This process is iterative: we start with an initial belief (the <strong>prior</strong>), update it with evidence (the <strong>likelihood</strong>), and obtain a revised belief (the <strong>posterior</strong>). The posterior then serves as the new prior for the next round of updates. Let’s see how this works in practice by performing <strong>Bayesian updating</strong> in Python.</p> <p>Below is a Python implementation that demonstrates the sequential nature of this process over 10 updates:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Bayesian updating
</span><span class="k">def</span> <span class="nf">bayesian_update</span><span class="p">(</span><span class="n">bayes_df</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
<span class="err"> </span> <span class="err"> </span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
<span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="n">bayes_df</span> <span class="o">=</span> <span class="n">bayes_df</span><span class="p">[[</span><span class="sh">'</span><span class="s">theta</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">posterior</span><span class="sh">'</span><span class="p">]].</span><span class="nf">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">posterior</span><span class="sh">'</span><span class="p">:</span><span class="sh">'</span><span class="s">prior</span><span class="sh">'</span><span class="p">})</span>
<span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="n">bayes_df</span><span class="p">[</span><span class="sh">'</span><span class="s">likelihood</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="nf">comb</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">nDisease</span><span class="p">)</span><span class="o">*</span><span class="n">theta</span><span class="o">**</span><span class="n">nDisease</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span><span class="o">**</span><span class="n">nHealthy</span>
<span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="n">marginal</span> <span class="o">=</span> <span class="p">(</span><span class="n">bayes_df</span><span class="p">[</span><span class="sh">'</span><span class="s">likelihood</span><span class="sh">'</span><span class="p">]</span><span class="o">*</span><span class="n">bayes_df</span><span class="p">[</span><span class="sh">'</span><span class="s">prior</span><span class="sh">'</span><span class="p">]).</span><span class="nf">sum</span><span class="p">()</span>
<span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="n">bayes_df</span><span class="p">[</span><span class="sh">'</span><span class="s">posterior</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">bayes_df</span><span class="p">[</span><span class="sh">'</span><span class="s">likelihood</span><span class="sh">'</span><span class="p">]</span><span class="o">*</span><span class="n">bayes_df</span><span class="p">[</span><span class="sh">'</span><span class="s">prior</span><span class="sh">'</span><span class="p">])</span><span class="o">/</span><span class="n">marginal</span>
<span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="n">posteriorMean</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">bayes_df</span><span class="p">[</span><span class="sh">'</span><span class="s">posterior</span><span class="sh">'</span><span class="p">]</span> <span class="o">*</span> <span class="n">bayes_df</span><span class="p">[</span><span class="sh">'</span><span class="s">theta</span><span class="sh">'</span><span class="p">])</span>
<span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="err"> </span> <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Posterior mean after </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s"> update is: </span><span class="si">{</span><span class="nf">round</span><span class="p">(</span><span class="n">posteriorMean</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="err"> </span> <span class="err"> </span> <span class="k">return</span> <span class="n">bayes_df</span>

<span class="n">bayes_update_df</span> <span class="o">=</span> <span class="nf">bayesian_update</span><span class="p">(</span><span class="n">bayes_df</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div> <p>Running the above code produces the following results, showing how the <strong>posterior mean</strong> changes after each update:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Posterior</span> <span class="n">mean</span> <span class="n">after</span> <span class="mi">1</span> <span class="n">update</span> <span class="ow">is</span><span class="p">:</span> <span class="mf">0.136</span>
<span class="n">Posterior</span> <span class="n">mean</span> <span class="n">after</span> <span class="mi">2</span> <span class="n">update</span> <span class="ow">is</span><span class="p">:</span> <span class="mf">0.125</span>
<span class="n">Posterior</span> <span class="n">mean</span> <span class="n">after</span> <span class="mi">3</span> <span class="n">update</span> <span class="ow">is</span><span class="p">:</span> <span class="mf">0.119</span>
<span class="n">Posterior</span> <span class="n">mean</span> <span class="n">after</span> <span class="mi">4</span> <span class="n">update</span> <span class="ow">is</span><span class="p">:</span> <span class="mf">0.115</span>
<span class="n">Posterior</span> <span class="n">mean</span> <span class="n">after</span> <span class="mi">5</span> <span class="n">update</span> <span class="ow">is</span><span class="p">:</span> <span class="mf">0.113</span>
<span class="n">Posterior</span> <span class="n">mean</span> <span class="n">after</span> <span class="mi">6</span> <span class="n">update</span> <span class="ow">is</span><span class="p">:</span> <span class="mf">0.111</span>
<span class="n">Posterior</span> <span class="n">mean</span> <span class="n">after</span> <span class="mi">7</span> <span class="n">update</span> <span class="ow">is</span><span class="p">:</span> <span class="mf">0.11</span>
<span class="n">Posterior</span> <span class="n">mean</span> <span class="n">after</span> <span class="mi">8</span> <span class="n">update</span> <span class="ow">is</span><span class="p">:</span> <span class="mf">0.109</span>
<span class="n">Posterior</span> <span class="n">mean</span> <span class="n">after</span> <span class="mi">9</span> <span class="n">update</span> <span class="ow">is</span><span class="p">:</span> <span class="mf">0.108</span>
<span class="n">Posterior</span> <span class="n">mean</span> <span class="n">after</span> <span class="mi">10</span> <span class="n">update</span> <span class="ow">is</span><span class="p">:</span> <span class="mf">0.107</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/distributions_prior_posterior_10-480.webp 480w,/assets/img/distributions_prior_posterior_10-800.webp 800w,/assets/img/distributions_prior_posterior_10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/distributions_prior_posterior_10.jpg" class="img-fluid rounded z-depth-1 mx-auto d-block" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This illustrates the <strong>sequential nature of Bayesian updating</strong>. This iterative process is fundamental to <strong>Bayesian inference</strong> and demonstrates how our understanding of a parameter improves with each new piece of evidence, making Bayesian methods highly suitable for dynamic, real-world problems where data comes in sequentially over time.</p>]]></content><author><name></name></author><category term="ML-concepts"/><category term="statistics"/><category term="code"/><category term="probability"/><summary type="html"><![CDATA[an example of a blog post with some code]]></summary></entry></feed>